{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This tutorial is for how to train ESM on Secondary Structure Prediction task.\n",
    "\n",
    "#### First lets import the important modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from protbench import applications\n",
    "from protbench import models\n",
    "from protbench import embedder\n",
    "from protbench.utils import EmbeddingsDatasetFromDisk\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second step is to initialize our models which takes only two steps.\n",
    "* The first step is to initialize the wrapper object which contains utility functions that abstracts away the differences between pretrained models (e.g. Ankh, ESM, ProtTrans, etc...) so it can help you in using the same script for different models.\n",
    "* The second step is to initialize the model and the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrapper = applications.initialize_model_from_checkpoint(\"esm2\", \"esm2_650M\")\n",
    "wrapper.initialze_model_from_checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Third step is to load our dataset which is secondary structure prediction in this tutorial.\n",
    "* In this tutorial we will extract the embeddings of each sequence from ESM then feed it to a ConvBERT downstream model that's why we set `from_embeddings=True` because our inputs to ConvBERT are going to be the embeddings not the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssp_task = applications.SSP3(\"ssp3_casp12\", from_embeddings=True, tokenizer=wrapper.tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fourth step is to load our sequences and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seqs, y_train = ssp_task.load_train_data()\n",
    "val_seqs, y_val = ssp_task.load_eval_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fifth step is to load our tokenizer using this function.\n",
    "\n",
    "##### You might ask why not just do `wrapper.tokenizer`? What is the differences between `wrapper.tokenizer` and this function?\n",
    "\n",
    "##### The answer is that this function returns an objects that wraps `wrapper.tokenizer` and the reason for that is because some models might have non-huggingface tokenizer or some models needs extra preprocessing for the sequences before passing it to the tokenizer example for that is ProtTrans T5 where this model requires replacing \"U, Z, O, B\" amino acids to \"X\", so this wrapper tokenizer object is convinent because it does the necessary steps for each model (if there is any). Also in case of extracting embeddings we only need the `input_ids` only so we can do that easily here by setting `return_input_ids_only=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = wrapper.load_default_tokenization_function(return_input_ids_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sixth step is to initialize our embedding extractor, it will take just two steps.\n",
    "\n",
    "* The first step is to initialize an instance that just stores the paths of the train, validation and test embeddings.\n",
    "* The second step is to initialize the `ComputeEmbeddingWrapper` instance which expects the following:\n",
    "    * model: Your pretrained model which is ESM in this tutorial.\n",
    "    * tokenization_fn: Your tokenization function that should be `callable`\n",
    "    * forward_options: In case your model takes extra arguments other than the `input_ids`, you can pass it here in a dictionary and it will be passed to the model along with the `input_ids`.\n",
    "    * post_processing_function: If your model returns multiple outputs then we will need a function that takes these outputs and returns only the embeddings.\n",
    "    * device: target device.\n",
    "    * pad_token_id: Padding ID\n",
    "    * low_memory: If this is `True` then the `ComputeEmbeddingsWrapper` will save the embeddings on the disk, otherwise it will save it to the memory, and if it's `True` then `ComputeEmbeddingsWrapper` expects `save_directories` to have `SaveDirectories` instance as we did below.\n",
    "    * save_directories: Instance that stores the paths for the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dirs = embedder.SaveDirectories(parent_dir=\"./\", train_dir=\"ssp_train_embeddings\", validation_dir=\"ssp_val_embeddings\")\n",
    "\n",
    "embedding_extractor = embedder.ComputeEmbeddingsWrapper(\n",
    "    model=wrapper.model,\n",
    "    tokenization_fn=tokenizer,\n",
    "    forward_options={},\n",
    "    post_processing_function=wrapper.embeddings_postprocessing_fn,\n",
    "    device=\"cpu\",\n",
    "    pad_token_id=tokenizer.tokenizer.pad_token_id,\n",
    "    low_memory=True,\n",
    "    save_directories=save_dirs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now lets run it. You will find two directories that are created and you will find inside them the embeddings for each sequence.\n",
    "##### Each embeddings file will be saved with `.npy` extention and each file will have a numerical name. this is convenient for us when we load them from disk we will load them by index which will be their names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_extractor(train_seqs=train_seqs, val_seqs=val_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now lets load our downstream model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convbert_model = models.ConvBert(\n",
    "    input_dim=wrapper.embedding_dim,\n",
    "    nhead=4,\n",
    "    hidden_dim=wrapper.embedding_dim // 2,\n",
    "    num_layers=1,\n",
    "    kernel_size=7,\n",
    "    dropout=0.1,\n",
    "    pooling=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This function is just simple wrapper that connects all the models together.\n",
    "\n",
    "#### Lets explain each argument.\n",
    "* task: Your target task which is secondary structure in this case.\n",
    "* embedding_dim: Pretrained model embedding dimension.\n",
    "* from_embeddings: If the inputs are embeddings or sequences.\n",
    "* backbone: ESM model that we are using in this tutorial.\n",
    "* downstream_model: Our ConvBERT downstream model that we are using in this tutorial.\n",
    "* pooling: Pooling function if we are doing sequence classification/regression.\n",
    "* embedding_postprocessing_fn: Embedding post processing function if your pretrained model returns many outputs along with the embeddings.\n",
    "\n",
    "#### Why this function needs the task to be passed to it?\n",
    "* Each task contain a method that returns it's appropriate head (nn.Module) that will be used during training (try: `ssp_task.load_task_head(wrapper.embedding_dim)`).\n",
    "\n",
    "#### If we are extracting the embeddings why do we need to pass the pretrained model?\n",
    "* You can ignore this parameter in case you are loading from embeddings, we are just passing it to show all the expected parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = models.utils.initialize_model(\n",
    "    task=ssp_task,\n",
    "    embedding_dim=wrapper.embedding_dim,\n",
    "    from_embeddings=ssp_task.from_embeddings,\n",
    "    backbone=wrapper.model,\n",
    "    downstream_model=convbert_model,\n",
    "    pooling=None,\n",
    "    embedding_postprocessing_fn=wrapper.embeddings_postprocessing_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets initialize our dataset, it expects the path to the embeddings and the labels.\n",
    "\n",
    "##### What is shifting?\n",
    "* Some models has start of sentence and end of sentence tokens, if you do not want these token embeddings to be included while training you can slice them. In this case ESM uses start and end of sentence tokens so we removed it by setting `shift_left=1` and `shift_right=1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EmbeddingsDatasetFromDisk(\"ssp_train_embeddings\", y_train, shift_left=1, shift_right=1)\n",
    "val_dataset = EmbeddingsDatasetFromDisk(\"ssp_val_embeddings\", y_val, shift_left=1, shift_right=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We are using Huggingface Trainer because its easy to use and fits our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(output_dir=\"ssp_experiment\", metric_for_best_model=ssp_task.metric_for_best_model)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=final_model,\n",
    "    args=args,\n",
    "    data_collator=ssp_task.collate_fn,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=ssp_task.metrics_fn,\n",
    "    preprocess_logits_for_metrics=ssp_task.preprocessing_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We finished! Now go check out `ankh_tutorial.ipynb`, you will find this tutorial has exactly the same steps except the name of the model while loading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
